{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e0a83e9",
   "metadata": {},
   "source": [
    "## Loading model and predicting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5a28b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f91152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/temp/opt/anaconda3/lib/python3.8/site-packages (0.19.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35181e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /Users/temp/opt/anaconda3/lib/python3.8/site-packages (0.15.3)\r\n",
      "Requirement already satisfied: nltk>=3.1 in /Users/temp/opt/anaconda3/lib/python3.8/site-packages (from textblob) (3.6.1)\r\n",
      "Requirement already satisfied: tqdm in /Users/temp/opt/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (4.59.0)\r\n",
      "Requirement already satisfied: joblib in /Users/temp/opt/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (1.0.1)\r\n",
      "Requirement already satisfied: click in /Users/temp/opt/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (7.1.2)\r\n",
      "Requirement already satisfied: regex in /Users/temp/opt/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (2021.4.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6806c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading all requirements\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import pandas as pd\n",
    "import time\n",
    "from transformers import (AutoTokenizer, AutoModel, \n",
    "                          AutoModelForSequenceClassification, \n",
    "                          DataCollatorWithPadding, AdamW, get_scheduler,\n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          )\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string \n",
    "from dotenv import dotenv_values\n",
    "from flask import Flask, config, render_template, request,redirect,url_for,send_file\n",
    "import io\n",
    "import base64\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from io import StringIO\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd1b1145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up seed value\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "#init tokenizer\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bcf44d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "975ab30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "def load_model():\n",
    "    checkpoint = \"distilbert-base-uncased\"\n",
    "    PATH = \"toxic_distilBERT_multilabel.pt\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels = 6)\n",
    "    model.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36b31f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read tweets csv file. It as 2 columns -> tweets and cleaned_tweets\n",
    "def tokenize_csv(tweets):\n",
    "    #tweets = pd.read_csv(tweets_df)\n",
    "    # tokenize and encode sequences in the actual test set\n",
    "    sub_tokens = tokenizer.batch_encode_plus(tweets[\"cleaned_tweets\"].tolist(),\n",
    "                                         max_length = 200,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         truncation=True,\n",
    "                                         return_token_type_ids=False\n",
    "                                         )\n",
    "    sub_seq = torch.tensor(sub_tokens['input_ids'])\n",
    "    sub_mask = torch.tensor(sub_tokens['attention_mask'])\n",
    "    sub_data = TensorDataset(sub_seq, sub_mask)\n",
    "    batch_size = 32\n",
    "    sub_dataloader = DataLoader(sub_data,batch_size=batch_size)\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    return sub_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d626f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict \n",
    "# Measure how long the evaluation going to takes.\n",
    "categories = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "def predict_final(sub_dataloader,model):\n",
    "    t0 = time.time()\n",
    "    for step, batch in enumerate(sub_dataloader):\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            pass\n",
    "            # Calculate elapsed time in minutes\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, b_input_mask)\n",
    "            pred_probs = torch.sigmoid(outputs.logits)\n",
    "            if step == 0:\n",
    "                predictions = pred_probs.cpu().detach().numpy()\n",
    "            else:\n",
    "                predictions = np.append(predictions, pred_probs.cpu().detach().numpy(), axis=0)\n",
    "    \n",
    "    predictions_df = pd.DataFrame(predictions, columns = categories)\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "932c6336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculations(predictions_df):\n",
    "    \n",
    "    # creating new columns for different cateogries. it will have % values\n",
    "    for name in categories:\n",
    "        predictions_df[name+'_calc'] = predictions_df[name] * 100\n",
    "    \n",
    "    threshold = 4\n",
    "    \n",
    "    toxics = predictions_df[predictions_df['toxic_calc']>threshold]['toxic_calc'].count() \n",
    "    severe_toxic = predictions_df[predictions_df['severe_toxic_calc']>threshold]['severe_toxic_calc'].count()\n",
    "    obscenes = predictions_df[predictions_df['obscene_calc']>threshold]['obscene_calc'].count()\n",
    "    threats = predictions_df[predictions_df['threat_calc']>threshold]['threat_calc'].count()\n",
    "    insults = predictions_df[predictions_df['insult_calc']>threshold]['insult_calc'].count()\n",
    "    identity_hates = predictions_df[predictions_df['identity_hate_calc']>threshold]['identity_hate_calc'].count()\n",
    "    \n",
    "    identified_num = [toxics]\n",
    "    identified_num.append(severe_toxic)\n",
    "    identified_num.append(obscenes)\n",
    "    identified_num.append(threats)\n",
    "    identified_num.append(insults)\n",
    "    identified_num.append(identity_hates)\n",
    "    \n",
    "    \n",
    "    #print(toxics)\n",
    "    #print(predictions_df['toxic'].count())\n",
    "    toxics_perc = toxics/predictions_df['toxic'].count() * 100\n",
    "    severe_toxic_perc = severe_toxic/predictions_df['severe_toxic'].count() * 100\n",
    "    obscene_perc = obscenes/predictions_df['obscene'].count() * 100\n",
    "    threat_perc = threats/predictions_df['threat'].count() * 100\n",
    "    insult_perc = insults/predictions_df['insult'].count() * 100\n",
    "    identity_hate_perc = identity_hates/predictions_df['identity_hate'].count() * 100\n",
    "    \n",
    "    identified_perc = [toxics_perc]\n",
    "    identified_perc.append(severe_toxic_perc)\n",
    "    identified_perc.append(obscene_perc)\n",
    "    identified_perc.append(threat_perc)\n",
    "    identified_perc.append(insult_perc)\n",
    "    identified_perc.append(identity_hate_perc)\n",
    "    \n",
    "    return predictions_df['toxic'].count(), identified_num, identified_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e80a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18616e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20614fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d27c9da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccca278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f6b170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f551d66",
   "metadata": {},
   "source": [
    "##  grab tweets using tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f642a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth(config):\n",
    "    auth = tweepy.OAuthHandler(config['CONSUMER_KEY'],config['CONSUMER_SECRET'])\n",
    "    auth.set_access_token(config['ACCESS_KEY'],config['ACCESS_SECRET'])\n",
    "    auth.secret = True\n",
    "    global api\n",
    "    api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "    return api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e89860eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "        text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        text = re.sub(r\"www.\\S+\", \"\", text)\n",
    "        text = re.sub(r'RT[\\s]+','',text)\n",
    "        text = re.sub('[0-9]+', '', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00d21b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets(df):\n",
    "        df['cleaned_tweets'] = df['tweets'].apply(lambda x: clean(str(x)))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4233ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grab_tweets(user):\n",
    "    tweetsPerQry = 100\n",
    "    fName = 'static/files/'+user+'.txt'\n",
    "    sinceId = None\n",
    "    max_id = -1\n",
    "    maxTweets = 1000\n",
    "    tweetCount = 0\n",
    "    output = []\n",
    "    print(\"Downloading max {0} tweets\",format(maxTweets))\n",
    "    with open(fName,'w') as f:\n",
    "        while tweetCount < maxTweets:\n",
    "            try:\n",
    "                if(max_id<=0):\n",
    "                    if(not sinceId):\n",
    "                        new_tweets = api.user_timeline(screen_name=user,lang='en',count=tweetsPerQry, tweet_mode ='extended')\n",
    "                    else:\n",
    "                        new_tweets = api.user_timeline(screen_name=user,lang='en',count=tweetsPerQry,since_id=sinceId, tweet_mode ='extended')\n",
    "                else:\n",
    "                    if(not sinceId):\n",
    "                        new_tweets = api.user_timeline(screen_name=user,lang='en',count=tweetsPerQry,max_id = str(max_id-1), tweet_mode ='extended')\n",
    "                    else:\n",
    "                        new_tweets = api.user_timeline(screen_name=user,lang='en',count=tweetsPerQry,max_id = str(max_id-1),since_id=sinceId, tweet_mode ='extended')\n",
    "    \n",
    "                if not new_tweets:\n",
    "                    print(\"No more tweets found\")\n",
    "                    break\n",
    "                for tweet in new_tweets:\n",
    "                    output.append(tweet.full_text.replace('\\n','').encode(\"utf-8\"))\n",
    "                    f.write(str(tweet.full_text.replace('\\n','').encode(\"utf-8\"))+\"\\n\")\n",
    "\n",
    "                tweetCount += len(new_tweets)\n",
    "                #print(\"Dowloaded {0} tweets\".format(tweetCount))\n",
    "                max_id=new_tweets[-1].id\n",
    "            except tweepy.TweepError as e:\n",
    "                print('some error: '+str(e))\n",
    "                break\n",
    "    df = pd.DataFrame(output,columns=['tweets'])  \n",
    "    return read_tweets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea34672",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values('.env')\n",
    "auth(config)\n",
    "tweet_df = grab_tweets(userName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef67dac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d0532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8806b8ae",
   "metadata": {},
   "source": [
    "## flask server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419db290",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Flow of program\n",
    "Run every cell first -> init torch -> load_model -> auth tweepy -> grab_tweets -> read_tweets -> clean_tweets \n",
    "tokenize_csv -> predict_final -> calculations \n",
    "\n",
    "Flask server should be running in this only \n",
    "Server start -> render home page -> get userName -> pass it to grab_tweets ->{next steps as above} -> pass back \n",
    "calcuations output to chart on front page\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13130b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# loading model. Tokenizer already fired up\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75288cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity():\n",
    "    polarity=lambda x:TextBlob(x).sentiment.polarity\n",
    "    subjectivity = lambda x:TextBlob(x).sentiment.subjectivity\n",
    "\n",
    "    tweet_polarity = np.zeros(len(tweet_df['cleaned_tweets']))\n",
    "    tweet_subjectivity = np.zeros(len(tweet_df['cleaned_tweets']))\n",
    "\n",
    "    for idx, tweet in enumerate(tweet_df['cleaned_tweets']):\n",
    "        tweet_polarity[idx] = polarity(tweet)\n",
    "        tweet_subjectivity[idx] = subjectivity(tweet)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.scatterplot(tweet_polarity, #x-axis\n",
    "                tweet_subjectivity, #y-axis\n",
    "                s=100)\n",
    "    \n",
    "    plt.title('sentimental Analysis', fontsize=20)\n",
    "    plt.xlabel('<-Negative- - - - - - - - - - - - - - - - - - - - - - - - - - - -Positive->',fontsize=15)\n",
    "    plt.ylabel('<-Facts- - - - - - - - - - - - - - - - -opinion->',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.plot()\n",
    "    \n",
    "    #save_file\n",
    "    url1 = 'static/images/'+name+'scatter.png'\n",
    "    my_file = Path(url1)\n",
    "    if my_file.exists():\n",
    "        pass\n",
    "    else:\n",
    "        plt.savefig(url1)\n",
    "    plt.close()\n",
    "    # second plot\n",
    "    f, axs = plt.subplots(1, 2, figsize=(20,8))\n",
    "\n",
    "    sns.distplot(tweet_polarity, color=\"b\", ax=axs[0])\n",
    "    axs[0].set_title(\"Tweet Polarity\", fontsize = 20)\n",
    "    axs[0].set_xlabel('← Negative - - - - - - - - - - - - - - - - - - Positive →', fontsize=15)\n",
    "    sns.distplot(tweet_subjectivity, color=\"b\", ax=axs[1])\n",
    "    axs[1].set_title(\"Tweet Subjectivity\", fontsize = 20)\n",
    "    axs[1].set_xlabel('← Facts - - - -  - - - - - - - - - - - - - - Opinions →', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.plot()\n",
    "    url2 = 'static/images/'+name+'dist.png'\n",
    "    my_file2 = Path(url2)\n",
    "    if my_file2.exists():\n",
    "        pass\n",
    "    else:\n",
    "        plt.savefig(url2)\n",
    "    plt.close()\n",
    "    \n",
    "    return url1,url2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c295c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [15/Oct/2021 17:03:36] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "Unexpected parameter: lang\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading max {0} tweets 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected parameter: lang\n",
      "Unexpected parameter: lang\n",
      "Unexpected parameter: lang\n",
      "Unexpected parameter: lang\n",
      "Unexpected parameter: lang\n",
      "Unexpected parameter: lang\n",
      "Unexpected parameter: lang\n",
      "Unexpected parameter: lang\n",
      "Unexpected parameter: lang\n",
      "Unexpected parameter: lang\n",
      "/Users/temp/opt/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/temp/opt/anaconda3/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n",
      "/Users/temp/opt/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/temp/opt/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "127.0.0.1 - - [15/Oct/2021 17:09:34] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static/images/charliekirk11scatter.png static/images/charliekirk11dist.png\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route('/',methods=['POST','GET'])\n",
    "def home():\n",
    "    if request.method=='POST':\n",
    "        global name\n",
    "        name = request.form['userName'].strip()\n",
    "        global tweet_df\n",
    "        tweet_df = grab_tweets(name) # will return a dataframe which has 2 columns named tweets and cleaned_tweets\n",
    "        #Now we have to tokenize it and predict\n",
    "        sub_dataloader = tokenize_csv(tweet_df)\n",
    "        predictions_df= predict_final(sub_dataloader,model)\n",
    "        length,identified_num, identified_perc = calculations(predictions_df)\n",
    "        \n",
    "        g1,g2 = polarity()\n",
    "        fName = 'static/images/'+name+'.txt'\n",
    "        print(g1,g2)\n",
    "        categories = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "        return render_template('predict.html',userName=name,\n",
    "                               nums = identified_num, perc=identified_perc, \n",
    "                               length =length, labels = categories, \n",
    "                               url1=g1, url2 = g2,fName = fName)\n",
    "    \n",
    "    else:\n",
    "        return render_template('home.html')\n",
    "        \n",
    "@app.route('/predict')\n",
    "def predict():\n",
    "    return render_template('predict.html')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config = dotenv_values('.env')\n",
    "    auth(config)\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e722a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed67d8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9da888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da468b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e07d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0775802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aef4090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = Path(\"static/images/charliekirk11scattjjer.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6d0fbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not\n"
     ]
    }
   ],
   "source": [
    "if my_file.exists():\n",
    "    print(\"already\")\n",
    "else:\n",
    "    print(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf85ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec1093f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1499e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d2fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b456f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2adaac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df28e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc83e54c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e1e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06ab429",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0562549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82463c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f6a482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98520365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee928730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrap code\n",
    "# firing up torch \n",
    "initialize_torch()\n",
    "load_model()\n",
    "sub_dataloader = tokenize_csv(tweet_df)\n",
    "predictions_df= predict_final(sub_dataloader)\n",
    "calculations(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12bbc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample accounts\n",
    "'''\n",
    "RyanAFournier\n",
    "charliekirk11\n",
    "TheHRH\n",
    "\n",
    "imillhiser\n",
    "owillis\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
